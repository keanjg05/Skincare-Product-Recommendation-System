{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Create Training Pairs (Query -> Product -> Label)\n",
        "\n",
        "This creates the supervised learning dataset for the neural network"
      ],
      "metadata": {
        "id": "Aza8IjidzKeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NKZVGrlxzgrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuration**"
      ],
      "metadata": {
        "id": "eNshK1VDzKp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "POSITIVES_PER_QUERY = 15  # Target positive examples per query\n",
        "NEGATIVES_PER_QUERY = 30  # Target negative examples per query\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "np.random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "IdYysWJy95Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data**"
      ],
      "metadata": {
        "id": "aPKmzf-s97FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLoading data...\")\n",
        "products_df = pd.read_csv('/content/drive/MyDrive/cosmetic_p.csv')\n",
        "queries_df = pd.read_csv('/content/drive/MyDrive/synthetic_queries_enhanced.csv')\n",
        "\n",
        "print(f\"Loaded {len(products_df)} products\")\n",
        "print(f\"Loaded {len(queries_df)} queries\")\n",
        "print(f\"  - Structured: {(queries_df['type']=='structured').sum()}\")\n",
        "print(f\"  - Realistic:  {(queries_df['type']=='realistic').sum()}\")"
      ],
      "metadata": {
        "id": "--oPoXSezmes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concern & Ingredient Mappings**"
      ],
      "metadata": {
        "id": "7z6-OQgezK0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSetting up concern-ingredient mappings...\")\n",
        "\n",
        "# Expanded concern detection\n",
        "CONCERN_KEYWORDS = {\n",
        "    'acne': ['acne', 'breakout', 'breaking out', 'pimple', 'blemish', 'blackhead',\n",
        "             'whitehead', 'clogged pores', 'congested'],\n",
        "    'aging': ['aging', 'wrinkle', 'fine line', 'anti-age', 'firm', 'sagging',\n",
        "              'mature skin', 'age spot', 'loss of elasticity'],\n",
        "    'hydration': ['dry', 'hydrat', 'moisture', 'dehydrat', 'flaky', 'tight',\n",
        "                  'parched', 'thirsty skin'],\n",
        "    'brightening': ['brighten', 'dull', 'glow', 'radiant', 'dark spot',\n",
        "                    'hyperpigmentation', 'uneven tone', 'discoloration'],\n",
        "    'redness': ['red', 'redness', 'irritat', 'inflam', 'blotchy', 'flush'],\n",
        "    'texture': ['texture', 'rough', 'smooth', 'bumpy', 'uneven', 'pores', 'large pores'],\n",
        "    'oiliness': ['oily', 'greasy', 'shiny', 'sebum', 'oil control'],\n",
        "    'sensitivity': ['sensitive', 'reactive', 'calm', 'sooth', 'gentle']\n",
        "}\n",
        "\n",
        "# Ingredient-to-concern mapping (expanded based on cosmetic science)\n",
        "INGREDIENT_CONCERNS = {\n",
        "    'acne': [\n",
        "        'salicylic acid', 'benzoyl peroxide', 'niacinamide', 'tea tree',\n",
        "        'sulfur', 'azelaic acid', 'zinc', 'retinol', 'adapalene'\n",
        "    ],\n",
        "    'aging': [\n",
        "        'retinol', 'retinoid', 'vitamin c', 'peptide', 'hyaluronic acid',\n",
        "        'adenosine', 'bakuchiol', 'coenzyme q10', 'resveratrol', 'matrixyl'\n",
        "    ],\n",
        "    'hydration': [\n",
        "        'hyaluronic acid', 'glycerin', 'ceramide', 'squalane', 'urea',\n",
        "        'panthenol', 'sodium hyaluronate', 'beta-glucan', 'allantoin'\n",
        "    ],\n",
        "    'brightening': [\n",
        "        'vitamin c', 'niacinamide', 'kojic acid', 'alpha arbutin',\n",
        "        'licorice', 'tranexamic acid', 'ascorbic acid', 'azelaic acid'\n",
        "    ],\n",
        "    'redness': [\n",
        "        'centella', 'cica', 'allantoin', 'bisabolol', 'azulene',\n",
        "        'colloidal oatmeal', 'green tea', 'chamomile', 'madecassoside'\n",
        "    ],\n",
        "    'texture': [\n",
        "        'glycolic acid', 'lactic acid', 'aha', 'bha', 'salicylic acid',\n",
        "        'retinol', 'enzyme', 'papaya', 'pineapple'\n",
        "    ],\n",
        "    'oiliness': [\n",
        "        'niacinamide', 'salicylic acid', 'zinc', 'clay', 'charcoal',\n",
        "        'witch hazel', 'tea tree', 'silica'\n",
        "    ],\n",
        "    'sensitivity': [\n",
        "        'centella', 'aloe', 'colloidal oatmeal', 'bisabolol', 'allantoin',\n",
        "        'panthenol', 'chamomile', 'calendula', 'madecassoside'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"Configured {len(CONCERN_KEYWORDS)} concern categories\")\n",
        "print(f\"Mapped {sum(len(v) for v in INGREDIENT_CONCERNS.values())} key ingredients\")"
      ],
      "metadata": {
        "id": "SYeL-RARzuLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper Functions**"
      ],
      "metadata": {
        "id": "gSgFab8ezK6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_skin_types_from_query(query):\n",
        "    \"\"\"Extract mentioned skin types from query\"\"\"\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    skin_types = {\n",
        "        'Dry': 1 if 'dry' in query_lower else 0,\n",
        "        'Oily': 1 if 'oily' in query_lower else 0,\n",
        "        'Combination': 1 if 'combination' in query_lower else 0,\n",
        "        'Normal': 1 if 'normal' in query_lower else 0,\n",
        "        'Sensitive': 1 if 'sensitive' in query_lower else 0\n",
        "    }\n",
        "\n",
        "    num_types = sum(skin_types.values())\n",
        "    return skin_types, num_types\n",
        "\n",
        "def extract_concerns_from_query(query):\n",
        "    \"\"\"Extract skincare concerns from query\"\"\"\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    concerns = {}\n",
        "    for concern, keywords in CONCERN_KEYWORDS.items():\n",
        "        concerns[concern] = any(keyword in query_lower for keyword in keywords)\n",
        "\n",
        "    num_concerns = sum(concerns.values())\n",
        "    return concerns, num_concerns\n",
        "\n",
        "def product_has_concern_ingredients(product_ingredients, concerns):\n",
        "    \"\"\"\n",
        "    Check if product has ingredients addressing the query's concerns.\n",
        "    Returns (has_match, matched_concerns)\n",
        "    \"\"\"\n",
        "    if not any(concerns.values()):\n",
        "        return False, []\n",
        "\n",
        "    ingredients_lower = product_ingredients.lower()\n",
        "    matched_concerns = []\n",
        "\n",
        "    for concern, is_mentioned in concerns.items():\n",
        "        if is_mentioned:\n",
        "            relevant_ingredients = INGREDIENT_CONCERNS.get(concern, [])\n",
        "            if any(ing in ingredients_lower for ing in relevant_ingredients):\n",
        "                matched_concerns.append(concern)\n",
        "\n",
        "    return len(matched_concerns) > 0, matched_concerns\n",
        "\n",
        "def compute_compatibility_score(product_row, query_skin_types, query_concerns):\n",
        "    \"\"\"\n",
        "    Compute a compatibility score for product-query pair.\n",
        "    FIXED: More lenient skin type matching + filters products with no skin type data.\n",
        "\n",
        "    Returns:\n",
        "        label (0 or 1): Binary compatibility\n",
        "        score (0-1): Confidence score for ranking\n",
        "        reason (str): Why it's compatible/incompatible\n",
        "    \"\"\"\n",
        "    reasons = []\n",
        "    score = 0.0\n",
        "\n",
        "    # Check: Product has skin type data\n",
        "    product_skin_types = sum([product_row[st] for st in ['Dry', 'Oily', 'Combination', 'Normal', 'Sensitive']])\n",
        "\n",
        "    if product_skin_types == 0:\n",
        "        # Product has no skin type data - automatic negative with low score\n",
        "        return 0, 0.0, \"No skin type data available\"\n",
        "\n",
        "    # SKIN TYPE COMPATIBILITY (Weight: 0.6)\n",
        "    num_query_types = sum(query_skin_types.values())\n",
        "\n",
        "    if num_query_types > 0:\n",
        "        # Count how many of the query's skin types the product supports\n",
        "        matching_types = sum(\n",
        "            product_row[skin_type]\n",
        "            for skin_type, required in query_skin_types.items()\n",
        "            if required == 1\n",
        "        )\n",
        "\n",
        "        # More lenient scoring\n",
        "        # If product matches AT LEAST ONE query skin type, give partial credit\n",
        "        if matching_types > 0:\n",
        "            # Give base credit for any match, plus bonus for matching more\n",
        "            skin_type_score = 0.5 + (matching_types / num_query_types) * 0.5\n",
        "            score += skin_type_score * 0.6\n",
        "\n",
        "            matched = [st for st, req in query_skin_types.items()\n",
        "                      if req == 1 and product_row[st] == 1]\n",
        "            reasons.append(f\"Matches {matching_types}/{num_query_types} skin types: {', '.join(matched)}\")\n",
        "        else:\n",
        "            reasons.append(\"No skin type match\")\n",
        "    else:\n",
        "        # No skin type specified - give neutral score\n",
        "        score += 0.3\n",
        "        reasons.append(\"No skin type specified (neutral)\")\n",
        "\n",
        "    # CONCERN COMPATIBILITY (Weight: 0.4)\n",
        "    num_query_concerns = sum(query_concerns.values())\n",
        "\n",
        "    if num_query_concerns > 0:\n",
        "        has_ingredients, matched_concerns = product_has_concern_ingredients(\n",
        "            product_row['ingredients'],\n",
        "            query_concerns\n",
        "        )\n",
        "\n",
        "        if has_ingredients:\n",
        "            concern_score = len(matched_concerns) / num_query_concerns\n",
        "            score += concern_score * 0.4\n",
        "            reasons.append(f\"Addresses concerns: {', '.join(matched_concerns)}\")\n",
        "        else:\n",
        "            reasons.append(\"No relevant ingredients for concerns\")\n",
        "    else:\n",
        "        # No concerns specified - give neutral score\n",
        "        score += 0.2\n",
        "        reasons.append(\"No concerns specified\")\n",
        "\n",
        "    #Final labeling decision\n",
        "    # fixed: Lower threshold to allow partial matches\n",
        "    label = 1 if score >= 0.45 else 0\n",
        "\n",
        "    reason_str = \" | \".join(reasons)\n",
        "\n",
        "    return label, score, reason_str"
      ],
      "metadata": {
        "id": "zHltVShnz165"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Training Pairs with Balanced Sampling**"
      ],
      "metadata": {
        "id": "aq_toaQNzK-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_pairs = []\n",
        "queries_processed = 0\n",
        "queries_with_no_positives = 0\n",
        "queries_with_no_negatives = 0\n",
        "\n",
        "print(f\"\\nTarget: {POSITIVES_PER_QUERY} positives + {NEGATIVES_PER_QUERY} negatives per query\")\n",
        "print(f\"   Expected total: ~{len(queries_df) * (POSITIVES_PER_QUERY + NEGATIVES_PER_QUERY):,} pairs\\n\")\n",
        "\n",
        "for idx, query_row in queries_df.iterrows():\n",
        "    query = query_row['query']\n",
        "    query_type = query_row['type']\n",
        "\n",
        "    # Extract query features\n",
        "    query_skin_types, num_skin_types = extract_skin_types_from_query(query)\n",
        "    query_concerns, num_concerns = extract_concerns_from_query(query)\n",
        "\n",
        "    # Compute compatibility for ALL products\n",
        "    positive_candidates = []\n",
        "    negative_candidates = []\n",
        "\n",
        "    for prod_idx, product_row in products_df.iterrows():\n",
        "        label, score, reason = compute_compatibility_score(\n",
        "            product_row, query_skin_types, query_concerns\n",
        "        )\n",
        "\n",
        "        pair = {\n",
        "            'query': query,\n",
        "            'query_type': query_type,\n",
        "            'query_has_skin_type': 1 if num_skin_types > 0 else 0,\n",
        "            'query_has_concern': 1 if num_concerns > 0 else 0,\n",
        "            'product_id': prod_idx,\n",
        "            'product_name': product_row['name'],\n",
        "            'product_brand': product_row['brand'],\n",
        "            'product_price': product_row['price'],\n",
        "            'product_rank': product_row['rank'],\n",
        "            'product_ingredients': product_row['ingredients'],\n",
        "            'label': label,\n",
        "            'compatibility_score': score,\n",
        "            'match_reason': reason\n",
        "        }\n",
        "\n",
        "        if label == 1:\n",
        "            positive_candidates.append(pair)\n",
        "        else:\n",
        "            negative_candidates.append(pair)\n",
        "\n",
        "    # BALANCED SAMPLING\n",
        "    # Sample positives (prioritize higher scores)\n",
        "    if len(positive_candidates) > 0:\n",
        "        # Sort by score and take top ones\n",
        "        positive_candidates.sort(key=lambda x: x['compatibility_score'], reverse=True)\n",
        "        num_pos_to_sample = min(len(positive_candidates), POSITIVES_PER_QUERY)\n",
        "        sampled_positives = positive_candidates[:num_pos_to_sample]\n",
        "        training_pairs.extend(sampled_positives)\n",
        "    else:\n",
        "        queries_with_no_positives += 1\n",
        "\n",
        "    # Sample negatives (random sampling, but avoid very low scores that might be noise)\n",
        "    if len(negative_candidates) > 0:\n",
        "        # Filter out products with score very close to 0.5 (ambiguous)\n",
        "        clear_negatives = [p for p in negative_candidates if p['compatibility_score'] < 0.4]\n",
        "\n",
        "        if len(clear_negatives) >= NEGATIVES_PER_QUERY:\n",
        "            sampled_negatives = np.random.choice(\n",
        "                clear_negatives,\n",
        "                NEGATIVES_PER_QUERY,\n",
        "                replace=False\n",
        "            ).tolist()\n",
        "        else:\n",
        "            # If not enough clear negatives, use all clear + some ambiguous\n",
        "            sampled_negatives = clear_negatives\n",
        "            remaining_needed = NEGATIVES_PER_QUERY - len(clear_negatives)\n",
        "            if remaining_needed > 0:\n",
        "                ambiguous = [p for p in negative_candidates if p['compatibility_score'] >= 0.4]\n",
        "                if len(ambiguous) > 0:\n",
        "                    extra = np.random.choice(\n",
        "                        ambiguous,\n",
        "                        min(remaining_needed, len(ambiguous)),\n",
        "                        replace=False\n",
        "                    ).tolist()\n",
        "                    sampled_negatives.extend(extra)\n",
        "\n",
        "        training_pairs.extend(sampled_negatives)\n",
        "    else:\n",
        "        queries_with_no_negatives += 1\n",
        "\n",
        "    queries_processed += 1\n",
        "\n",
        "    # Progress indicator\n",
        "    if (idx + 1) % 200 == 0:\n",
        "        print(f\"  Processed {idx + 1:,}/{len(queries_df):,} queries... \"\n",
        "              f\"({len(training_pairs):,} pairs created)\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "training_df = pd.DataFrame(training_pairs)\n",
        "\n",
        "print(f\"\\nCreated {len(training_df):,} training pairs from {queries_processed} queries!\")\n",
        "print(f\"   Queries with no positives: {queries_with_no_positives}\")\n",
        "print(f\"   Queries with no negatives: {queries_with_no_negatives}\")"
      ],
      "metadata": {
        "id": "UAUAiLS4z741"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyze Training Data**"
      ],
      "metadata": {
        "id": "VBKyOsTTzLCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label distribution\n",
        "print(\"\\nLabel Distribution:\")\n",
        "label_counts = training_df['label'].value_counts()\n",
        "print(f\"  Positive (compatible):   {label_counts.get(1, 0):,} ({label_counts.get(1, 0)/len(training_df)*100:.1f}%)\")\n",
        "print(f\"  Negative (incompatible): {label_counts.get(0, 0):,} ({label_counts.get(0, 0)/len(training_df)*100:.1f}%)\")\n",
        "\n",
        "positive_ratio = label_counts.get(1, 0) / len(training_df)\n",
        "if 0.25 <= positive_ratio <= 0.45:\n",
        "    print(\"  Dataset is well-balanced for training!\")\n",
        "elif positive_ratio < 0.25:\n",
        "    print(\"  Few positives - consider increasing POSITIVES_PER_QUERY\")\n",
        "else:\n",
        "    print(\"  Many positives - this is actually fine!\")\n",
        "\n",
        "# Score distribution\n",
        "print(\"\\nCompatibility Score Distribution:\")\n",
        "print(f\"  Positive pairs - Mean score: {training_df[training_df['label']==1]['compatibility_score'].mean():.3f}\")\n",
        "print(f\"  Negative pairs - Mean score: {training_df[training_df['label']==0]['compatibility_score'].mean():.3f}\")\n",
        "\n",
        "# Query type breakdown\n",
        "print(\"\\nDistribution by Query Type:\")\n",
        "for qtype in ['structured', 'realistic']:\n",
        "    subset = training_df[training_df['query_type'] == qtype]\n",
        "    if len(subset) > 0:\n",
        "        pos = (subset['label'] == 1).sum()\n",
        "        neg = (subset['label'] == 0).sum()\n",
        "        print(f\"  {qtype.capitalize():12s}: {len(subset):,} pairs ({pos:,} pos, {neg:,} neg)\")\n",
        "\n",
        "# Query features\n",
        "print(\"\\nQuery Features:\")\n",
        "with_skin = training_df[training_df['query_has_skin_type'] == 1]\n",
        "without_skin = training_df[training_df['query_has_skin_type'] == 0]\n",
        "print(f\"  With skin type:    {len(with_skin):,} pairs ({(with_skin['label']==1).sum():,} positive)\")\n",
        "print(f\"  Without skin type: {len(without_skin):,} pairs ({(without_skin['label']==1).sum():,} positive)\")\n",
        "\n",
        "with_concern = training_df[training_df['query_has_concern'] == 1]\n",
        "without_concern = training_df[training_df['query_has_concern'] == 0]\n",
        "print(f\"  With concern:      {len(with_concern):,} pairs ({(with_concern['label']==1).sum():,} positive)\")\n",
        "print(f\"  Without concern:   {len(without_concern):,} pairs ({(without_concern['label']==1).sum():,} positive)\")\n",
        "\n",
        "# Sample examples\n",
        "print(\"\\nSample POSITIVE Examples:\")\n",
        "positive_samples = training_df[training_df['label'] == 1].sample(min(5, len(training_df[training_df['label'] == 1])))\n",
        "for _, row in positive_samples.iterrows():\n",
        "    print(f\"\\n  Query: '{row['query']}'\")\n",
        "    print(f\"  Product: {row['product_name']} ({row['product_brand']})\")\n",
        "    print(f\"  Score: {row['compatibility_score']:.2f}\")\n",
        "    print(f\"  Reason: {row['match_reason']}\")\n",
        "\n",
        "print(\"\\nSample NEGATIVE Examples:\")\n",
        "negative_samples = training_df[training_df['label'] == 0].sample(min(5, len(training_df[training_df['label'] == 0])))\n",
        "for _, row in negative_samples.iterrows():\n",
        "    print(f\"\\n  Query: '{row['query']}'\")\n",
        "    print(f\"  Product: {row['product_name']} ({row['product_brand']})\")\n",
        "    print(f\"  Score: {row['compatibility_score']:.2f}\")\n",
        "    print(f\"  Reason: {row['match_reason']}\")"
      ],
      "metadata": {
        "id": "r7uh34Nb0CZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/Val/Test Split**"
      ],
      "metadata": {
        "id": "S54WUFx2zLIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split by query to prevent leakage\n",
        "unique_queries = training_df['query'].unique()\n",
        "print(f\"\\nTotal unique queries: {len(unique_queries)}\")\n",
        "\n",
        "train_queries, temp_queries = train_test_split(\n",
        "    unique_queries, test_size=0.3, random_state=RANDOM_SEED\n",
        ")\n",
        "val_queries, test_queries = train_test_split(\n",
        "    temp_queries, test_size=0.5, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(f\"  Train queries: {len(train_queries)} ({len(train_queries)/len(unique_queries)*100:.1f}%)\")\n",
        "print(f\"  Val queries:   {len(val_queries)} ({len(val_queries)/len(unique_queries)*100:.1f}%)\")\n",
        "print(f\"  Test queries:  {len(test_queries)} ({len(test_queries)/len(unique_queries)*100:.1f}%)\")\n",
        "\n",
        "train_df = training_df[training_df['query'].isin(train_queries)].reset_index(drop=True)\n",
        "val_df = training_df[training_df['query'].isin(val_queries)].reset_index(drop=True)\n",
        "test_df = training_df[training_df['query'].isin(test_queries)].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nSplit Sizes:\")\n",
        "print(f\"  Train: {len(train_df):,} pairs ({(train_df['label']==1).sum():,} positive, {(train_df['label']==1).sum()/len(train_df)*100:.1f}%)\")\n",
        "print(f\"  Val:   {len(val_df):,} pairs ({(val_df['label']==1).sum():,} positive, {(val_df['label']==1).sum()/len(val_df)*100:.1f}%)\")\n",
        "print(f\"  Test:  {len(test_df):,} pairs ({(test_df['label']==1).sum():,} positive, {(test_df['label']==1).sum()/len(test_df)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "zszFfSGn0VEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save Data**"
      ],
      "metadata": {
        "id": "ixn1SaXyzLM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save full dataset\n",
        "training_df.to_csv('training_pairs_optimized.csv', index=False)\n",
        "print(f\"Saved: training_pairs_optimized.csv ({len(training_df):,} pairs)\")\n",
        "\n",
        "# Save splits\n",
        "train_df.to_csv('training_pairs_train.csv', index=False)\n",
        "val_df.to_csv('training_pairs_val.csv', index=False)\n",
        "test_df.to_csv('training_pairs_test.csv', index=False)\n",
        "print(f\"Saved: training_pairs_train.csv ({len(train_df):,} pairs)\")\n",
        "print(f\"Saved: training_pairs_val.csv ({len(val_df):,} pairs)\")\n",
        "print(f\"Saved: training_pairs_test.csv ({len(test_df):,} pairs)\")\n",
        "\n",
        "# Save to Google Drive\n",
        "try:\n",
        "    training_df.to_csv('/content/drive/MyDrive/training_pairs_optimized.csv', index=False)\n",
        "    train_df.to_csv('/content/drive/MyDrive/training_pairs_train.csv', index=False)\n",
        "    val_df.to_csv('/content/drive/MyDrive/training_pairs_val.csv', index=False)\n",
        "    test_df.to_csv('/content/drive/MyDrive/training_pairs_test.csv', index=False)\n",
        "    print(f\"Saved all files to Google Drive\")\n",
        "except:\n",
        "    print(\"Could not save to Google Drive\")"
      ],
      "metadata": {
        "id": "IOtk6YQE0azN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizations**"
      ],
      "metadata": {
        "id": "dq6fkebB0XYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCreating visualizations...\")\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# 1. Label distribution\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "label_counts.plot(kind='bar', ax=ax1, color=['#e74c3c', '#2ecc71'])\n",
        "ax1.set_title('Label Distribution', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('Label')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.set_xticklabels(['Incompatible', 'Compatible'], rotation=0)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 2. Score distribution by label\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "training_df[training_df['label']==0]['compatibility_score'].hist(\n",
        "    bins=30, ax=ax2, alpha=0.6, color='#e74c3c', label='Negative'\n",
        ")\n",
        "training_df[training_df['label']==1]['compatibility_score'].hist(\n",
        "    bins=30, ax=ax2, alpha=0.6, color='#2ecc71', label='Positive'\n",
        ")\n",
        "ax2.set_title('Compatibility Score Distribution', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('Score')\n",
        "ax2.set_ylabel('Count')\n",
        "ax2.legend()\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 3. Query type breakdown\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "query_type_data = training_df.groupby(['query_type', 'label']).size().unstack()\n",
        "query_type_data.plot(kind='bar', ax=ax3, color=['#e74c3c', '#2ecc71'])\n",
        "ax3.set_title('Labels by Query Type', fontsize=12, fontweight='bold')\n",
        "ax3.set_xlabel('Query Type')\n",
        "ax3.set_ylabel('Count')\n",
        "ax3.legend(['Incompatible', 'Compatible'])\n",
        "ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45)\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 4. Split sizes\n",
        "ax4 = fig.add_subplot(gs[1, 0])\n",
        "split_sizes = [len(train_df), len(val_df), len(test_df)]\n",
        "split_labels = ['Train', 'Val', 'Test']\n",
        "colors = ['#3498db', '#9b59b6', '#e67e22']\n",
        "ax4.bar(split_labels, split_sizes, color=colors)\n",
        "ax4.set_title('Dataset Split Sizes', fontsize=12, fontweight='bold')\n",
        "ax4.set_ylabel('Number of Pairs')\n",
        "ax4.grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(split_sizes):\n",
        "    ax4.text(i, v + max(split_sizes)*0.02, f'{v:,}', ha='center', fontweight='bold')\n",
        "\n",
        "# 5. Positive ratio by split\n",
        "ax5 = fig.add_subplot(gs[1, 1])\n",
        "positive_ratios = [\n",
        "    (train_df['label']==1).sum() / len(train_df) * 100,\n",
        "    (val_df['label']==1).sum() / len(val_df) * 100,\n",
        "    (test_df['label']==1).sum() / len(test_df) * 100\n",
        "]\n",
        "ax5.bar(split_labels, positive_ratios, color=colors)\n",
        "ax5.set_title('Positive Ratio by Split', fontsize=12, fontweight='bold')\n",
        "ax5.set_ylabel('Positive %')\n",
        "ax5.axhline(y=33.3, color='gray', linestyle='--', alpha=0.5, label='Target: 33%')\n",
        "ax5.legend()\n",
        "ax5.grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(positive_ratios):\n",
        "    ax5.text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
        "\n",
        "# 6. Query features\n",
        "ax6 = fig.add_subplot(gs[1, 2])\n",
        "feature_data = pd.DataFrame({\n",
        "    'With Skin Type': [len(with_skin), (with_skin['label']==1).sum()],\n",
        "    'Without Skin Type': [len(without_skin), (without_skin['label']==1).sum()],\n",
        "    'With Concern': [len(with_concern), (with_concern['label']==1).sum()],\n",
        "    'Without Concern': [len(without_concern), (without_concern['label']==1).sum()]\n",
        "}, index=['Total', 'Positive'])\n",
        "feature_data.T.plot(kind='bar', ax=ax6, color=['#95a5a6', '#2ecc71'])\n",
        "ax6.set_title('Query Features', fontsize=12, fontweight='bold')\n",
        "ax6.set_ylabel('Count')\n",
        "ax6.legend(['Total', 'Positive'])\n",
        "ax6.set_xticklabels(ax6.get_xticklabels(), rotation=45, ha='right')\n",
        "ax6.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 7. Top brands in positive pairs\n",
        "ax7 = fig.add_subplot(gs[2, :])\n",
        "positive_brands = training_df[training_df['label']==1]['product_brand'].value_counts().head(15)\n",
        "positive_brands.plot(kind='barh', ax=ax7, color='#2ecc71')\n",
        "ax7.set_title('Top 15 Brands in Positive Pairs', fontsize=12, fontweight='bold')\n",
        "ax7.set_xlabel('Number of Compatible Pairs')\n",
        "ax7.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.savefig('training_data_analysis_optimized.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved: training_data_analysis_optimized.png\")"
      ],
      "metadata": {
        "id": "Ek91RMNg0iKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**"
      ],
      "metadata": {
        "id": "tEGIQLjp0lHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"\n",
        "SUMMARY:\n",
        "  * Total training pairs:    {len(training_df):,}\n",
        "  * Unique queries:          {len(unique_queries):,}\n",
        "  * Unique products:         {len(products_df):,}\n",
        "  * Positive examples:       {label_counts.get(1, 0):,} ({positive_ratio*100:.1f}%)\n",
        "  * Negative examples:       {label_counts.get(0, 0):,} ({(1-positive_ratio)*100:.1f}%)\n",
        "\n",
        "  * Train set:  {len(train_df):,} pairs from {len(train_queries)} queries\n",
        "  * Val set:    {len(val_df):,} pairs from {len(val_queries)} queries\n",
        "  * Test set:   {len(test_df):,} pairs from {len(test_queries)} queries\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "N5B3_Mv20lXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns in products_df:\")\n",
        "print(products_df.columns.tolist())\n",
        "\n",
        "# If there's a category column, show distribution\n",
        "if 'category' in products_df.columns:\n",
        "    print(\"\\nCategory distribution:\")\n",
        "    print(products_df['category'].value_counts())\n",
        "\n",
        "print(\"\\nLabel column values:\")\n",
        "print(products_df['Label'].value_counts())"
      ],
      "metadata": {
        "id": "PN-v34IWj2gQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}